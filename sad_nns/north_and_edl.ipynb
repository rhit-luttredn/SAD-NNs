{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luttredn/senior-research/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sad_nns.uncertainty import *\n",
    "from neurops import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTH:** Define a LeNet-style model. \n",
    "\n",
    "Use the `ModSequential` class to wrap the `ModConv2d` and `ModLinear` model, which allows us to mask, prune, and grow the model. \n",
    "\n",
    "Use the `track_activations` and `track_auxiliary_gradients` arguments to enable the tracking of activations and auxiliary gradients later. \n",
    "\n",
    "By adding the `input_shape` of the data, we can compute the conversion factor of how many input neurons to add to the first linear layer when a new output channel is added to the final convolutional layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModSequential(\n",
    "        ModConv2d(in_channels=1, out_channels=8, kernel_size=7, masked=True, padding=1, learnable_mask=True),\n",
    "        ModConv2d(in_channels=8, out_channels=16, kernel_size=7, masked=True, padding=1, prebatchnorm=True, learnable_mask=True),\n",
    "        ModConv2d(in_channels=16, out_channels=16, kernel_size=5, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(64, 32, masked=True, prebatchnorm=True, learnable_mask=True),\n",
    "        ModLinear(32, 10, masked=True, prebatchnorm=True, nonlinearity=\"\"),\n",
    "        track_activations=True,\n",
    "        track_auxiliary_gradients=True,\n",
    "        input_shape = (1, 14, 14)\n",
    "    ).to(device)\n",
    "torch.compile(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Add EDL Loss Function\n",
    "# KLDivergenceLoss, MaximumLikelihoodLoss, CrossEntropyBayesRisk, SquaredErrorBayesRisk\n",
    "criterion = SquaredErrorBayesRisk()\n",
    "kl_divergence = KLDivergenceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTH:** Get a dataset and define standard training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 85774580.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 43402255.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 23230217.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 8134299.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('../data/', train=True, download=True,\n",
    "                     transform=transforms.Compose([ \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ]))\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, lengths=[int(0.9*len(dataset)), int(0.1*len(dataset))])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                            transforms.Resize((14,14))\n",
    "                        ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, epochs=10, num_classes=10, val_loader=None, verbose=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Convert target to one-hot encoding\n",
    "            target = F.one_hot(target, num_classes=num_classes)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate uncertainty\n",
    "            evidence = F.relu(output)\n",
    "            loss = criterion(evidence, target)\n",
    "\n",
    "            # Calculate KL Divergence Loss\n",
    "            kl_divergence_loss = kl_divergence(evidence, target)\n",
    "            annealing_step = 10\n",
    "            annealing_coef = torch.min(\n",
    "                torch.tensor(1.0, dtype=torch.float32),\n",
    "                torch.tensor(epoch / annealing_step, dtype=torch.float32)\n",
    "            )\n",
    "            loss += annealing_coef * kl_divergence_loss\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0 and verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if val_loader is not None:\n",
    "            print(\"Validation: \", end = \"\")\n",
    "            test(model, val_loader, criterion)\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Convert target to one-hot encoding\n",
    "            target_one_hot = F.one_hot(target, num_classes=10).float()\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += criterion(output, target_one_hot).item()\n",
    "\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTH:** Pretrain the model before changing the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luttredn/senior-research/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.972798\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.878762\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.813855\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.789927\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.749602\n",
      "Validation: Average loss: 0.0050, Accuracy: 5010/6000 (83.50%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.801659\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.661272\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.423046\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.399071\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.312982\n",
      "Validation: Average loss: 0.0144, Accuracy: 5531/6000 (92.18%)\n",
      "Train Epoch: 2 [0/54000 (0%)]\tLoss: 0.347399\n",
      "Train Epoch: 2 [12800/54000 (24%)]\tLoss: 0.306728\n",
      "Train Epoch: 2 [25600/54000 (47%)]\tLoss: 0.264475\n",
      "Train Epoch: 2 [38400/54000 (71%)]\tLoss: 0.273152\n",
      "Train Epoch: 2 [51200/54000 (95%)]\tLoss: 0.236729\n",
      "Validation: Average loss: 0.0120, Accuracy: 5684/6000 (94.73%)\n",
      "Train Epoch: 3 [0/54000 (0%)]\tLoss: 0.222862\n",
      "Train Epoch: 3 [12800/54000 (24%)]\tLoss: 0.277992\n",
      "Train Epoch: 3 [25600/54000 (47%)]\tLoss: 0.195385\n",
      "Train Epoch: 3 [38400/54000 (71%)]\tLoss: 0.188837\n",
      "Train Epoch: 3 [51200/54000 (95%)]\tLoss: 0.157143\n",
      "Validation: Average loss: 0.0115, Accuracy: 5708/6000 (95.13%)\n",
      "Train Epoch: 4 [0/54000 (0%)]\tLoss: 0.226028\n",
      "Train Epoch: 4 [12800/54000 (24%)]\tLoss: 0.172120\n",
      "Train Epoch: 4 [25600/54000 (47%)]\tLoss: 0.143060\n",
      "Train Epoch: 4 [38400/54000 (71%)]\tLoss: 0.215242\n",
      "Train Epoch: 4 [51200/54000 (95%)]\tLoss: 0.255799\n",
      "Validation: Average loss: 0.0108, Accuracy: 5708/6000 (95.13%)\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, epochs=5, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NORTH:** Model Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTH:** Use a heuristic from `metrics.py` to measure the existing channels and neurons to determine which ones to prune.\n",
    "\n",
    "The simplest one is measuring the norm of incoming weights to a neuron. We'll copy the model (so we have access to the original), then score each neuron and prune the lowest scoring ones within each layer. After running the following block, try uncommenting different lines to see how different metrics affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 scores: mean 4.49, std 0.236, min 4.16, smallest 25%: [1 0]\n",
      "Layer 1 scores: mean 8.73, std 0.447, min 8, smallest 25%: [7 5 1 3]\n",
      "Layer 2 scores: mean 8.83, std 0.621, min 7.54, smallest 25%: [ 6 11  1 15]\n",
      "Layer 3 scores: mean 3.26, std 0.33, min 2.49, smallest 25%: [19  7 11 21 29 15 28 23]\n",
      "The pruned model has 9058 effective parameters.\n",
      "Validation after pruning: Average loss: 0.0097, Accuracy: 4111/6000 (68.52%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.747649\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.685697\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.659413\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.621350\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.612561\n",
      "Validation: Average loss: 0.0036, Accuracy: 5643/6000 (94.05%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.614553\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.399520\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.357692\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.307343\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.242675\n",
      "Validation: Average loss: 0.0166, Accuracy: 5688/6000 (94.80%)\n"
     ]
    }
   ],
   "source": [
    "modded_model = copy.deepcopy(model)\n",
    "modded_optimizer = torch.optim.SGD(modded_model.parameters(), lr=0.01)\n",
    "modded_optimizer.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for i in range(len(model)-1):\n",
    "    scores = weight_sum(modded_model[i].weight)\n",
    "    # scores = weight_sum(modded_model[i].weight) +  weight_sum(modded_model[i+1].weight, fanin=False, conversion_factor=model.conversion_factor if i == model.conversion_layer else -1)\n",
    "    # scores = activation_variance(modded_model.activations[str(i)])\n",
    "    # scores = svd_score(modded_model.activations[str(i)])\n",
    "    # scores = nuclear_score(modded_model.activations[str(i)], average=i<3)\n",
    "    # scores = modded_model[i+1].batchnorm.weight.abs() if i != modded_model.conversion_layer else modded_model[i+1].batchnorm.weight.abs().reshape(modded_model.conversion_factor,-1).sum(0) \n",
    "    # Before trying this line, run the following block: # scores = fisher_info(mask_grads[i])\n",
    "    print(\"Layer {} scores: mean {:.3g}, std {:.3g}, min {:.3g}, smallest 25%:\".format(i, scores.mean(), scores.std(), scores.min()), end=\" \")\n",
    "    to_prune = np.argsort(scores.detach().cpu().numpy())[:int(0.25*len(scores))]\n",
    "    print(to_prune)\n",
    "    modded_model.prune(i, to_prune, optimizer=modded_optimizer, clear_activations=True)\n",
    "print(\"The pruned model has {} effective parameters.\".format(modded_model.parameter_count(masked = True)))\n",
    "print(\"Validation after pruning: \", end = \"\")\n",
    "test(modded_model, val_loader, criterion)\n",
    "train(modded_model, train_loader, modded_optimizer, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NORTH:** Grow the model using a neurogenesis strategy similar to NORTH-Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 score: 8/8, neurons to add: 1\n",
      "Layer 1 score: 16/16, neurons to add: 1\n",
      "Layer 2 score: 16/16, neurons to add: 1\n",
      "Layer 3 score: 32/32, neurons to add: 2\n",
      "The grown model now has 16731 effective parameters.\n",
      "Validation after growing: Average loss: 0.0108, Accuracy: 5708/6000 (95.13%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.654126\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.593728\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.579987\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.567058\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.531281\n",
      "Validation: Average loss: 0.0028, Accuracy: 5755/6000 (95.92%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.585440\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.378533\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.316372\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.277143\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.250785\n",
      "Validation: Average loss: 0.0171, Accuracy: 5732/6000 (95.53%)\n",
      "Layer 0 score: 8/9, neurons to add: 0\n",
      "Layer 1 score: 17/17, neurons to add: 1\n",
      "Layer 2 score: 17/17, neurons to add: 1\n",
      "Layer 3 score: 34/34, neurons to add: 2\n",
      "The grown model now has 19143 effective parameters.\n",
      "Validation after growing: Average loss: 0.0171, Accuracy: 5732/6000 (95.53%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.544091\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.513225\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.503585\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.464649\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.462490\n",
      "Validation: Average loss: 0.0020, Accuracy: 5742/6000 (95.70%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.458819\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.329700\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.271266\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.270787\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.235631\n",
      "Validation: Average loss: 0.0400, Accuracy: 5729/6000 (95.48%)\n",
      "Layer 0 score: 9/9, neurons to add: 1\n",
      "Layer 1 score: 18/18, neurons to add: 1\n",
      "Layer 2 score: 18/18, neurons to add: 1\n",
      "Layer 3 score: 36/36, neurons to add: 2\n",
      "The grown model now has 20863 effective parameters.\n",
      "Validation after growing: Average loss: 0.0406, Accuracy: 5729/6000 (95.48%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.470437\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.435028\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.406646\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.420289\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.411202\n",
      "Validation: Average loss: 0.0020, Accuracy: 5763/6000 (96.05%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.424378\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.333624\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.329233\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.258877\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.275853\n",
      "Validation: Average loss: 0.0164, Accuracy: 5742/6000 (95.70%)\n",
      "Layer 0 score: 9/10, neurons to add: 0\n",
      "Layer 1 score: 19/19, neurons to add: 1\n",
      "Layer 2 score: 19/19, neurons to add: 1\n",
      "Layer 3 score: 38/38, neurons to add: 2\n",
      "The grown model now has 23552 effective parameters.\n",
      "Validation after growing: Average loss: 0.0164, Accuracy: 5742/6000 (95.70%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.446015\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.388205\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.403113\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.381895\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.384242\n",
      "Validation: Average loss: 0.0038, Accuracy: 5755/6000 (95.92%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.392719\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.295076\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.273840\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.239241\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.226088\n",
      "Validation: Average loss: 0.0177, Accuracy: 5761/6000 (96.02%)\n",
      "Layer 0 score: 9/10, neurons to add: 0\n",
      "Layer 1 score: 20/20, neurons to add: 1\n",
      "Layer 2 score: 20/20, neurons to add: 1\n",
      "Layer 3 score: 39/40, neurons to add: 1\n",
      "The grown model now has 25291 effective parameters.\n",
      "Validation after growing: Average loss: 0.0179, Accuracy: 5761/6000 (96.02%)\n",
      "Train Epoch: 0 [0/54000 (0%)]\tLoss: 0.391899\n",
      "Train Epoch: 0 [12800/54000 (24%)]\tLoss: 0.355566\n",
      "Train Epoch: 0 [25600/54000 (47%)]\tLoss: 0.337849\n",
      "Train Epoch: 0 [38400/54000 (71%)]\tLoss: 0.344268\n",
      "Train Epoch: 0 [51200/54000 (95%)]\tLoss: 0.344796\n",
      "Validation: Average loss: 0.0040, Accuracy: 5767/6000 (96.12%)\n",
      "Train Epoch: 1 [0/54000 (0%)]\tLoss: 0.379568\n",
      "Train Epoch: 1 [12800/54000 (24%)]\tLoss: 0.300253\n",
      "Train Epoch: 1 [25600/54000 (47%)]\tLoss: 0.290156\n",
      "Train Epoch: 1 [38400/54000 (71%)]\tLoss: 0.235666\n",
      "Train Epoch: 1 [51200/54000 (95%)]\tLoss: 0.252360\n",
      "Validation: Average loss: 0.0457, Accuracy: 5775/6000 (96.25%)\n"
     ]
    }
   ],
   "source": [
    "modded_model_grow = copy.deepcopy(model)\n",
    "modded_optimizer_grow = torch.optim.SGD(modded_model_grow.parameters(), lr=0.01)\n",
    "modded_optimizer_grow.load_state_dict(optimizer.state_dict())\n",
    "\n",
    "for iter in range(5):\n",
    "    for i in range(len(modded_model_grow)-1):\n",
    "        #score = orthogonality_gap(modded_model_grow.activations[str(i)])\n",
    "        max_rank = modded_model_grow[i].width()\n",
    "        score = effective_rank(modded_model_grow.activations[str(i)])\n",
    "        to_add = max(score-int(0.95*max_rank), 0)\n",
    "        print(\"Layer {} score: {}/{}, neurons to add: {}\".format(i, score, max_rank, to_add))\n",
    "        modded_model_grow.grow(i, to_add, fanin_weights=\"iterative_orthogonalization\",\n",
    "                               optimizer=modded_optimizer_grow)\n",
    "    print(\"The grown model now has {} effective parameters.\".format(modded_model_grow.parameter_count(masked = True)))\n",
    "    print(\"Validation after growing: \", end = \"\")\n",
    "    test(modded_model_grow, val_loader, criterion)\n",
    "    train(modded_model_grow, train_loader, modded_optimizer_grow, criterion, epochs=2, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
